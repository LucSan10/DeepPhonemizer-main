model:
  d_fft: 1024
  d_model: 512
  dropout: 0.1
  heads: 4
  layers: 6
  type: transformer
paths:
  checkpoint_dir: checkpoints
  data_dir: datasets
preprocessing:
  char_repeats: 3
  languages:
  - pt_br
  lowercase: false
  n_val: 2000
  phoneme_symbols:
  - ''''
  - .
  - E
  - J
  - L
  - O
  - R
  - S
  - X
  - Z
  - a
  - a~
  - b
  - d
  - dZ
  - e
  - ej
  - e~
  - e~j~
  - f
  - g
  - i
  - i~
  - j
  - js
  - j~
  - j~s
  - k
  - l
  - m
  - n
  - o
  - ow
  - o~
  - p
  - pau
  - r
  - s
  - t
  - tS
  - u
  - u~
  - v
  - w
  - w~
  - z
  - ' '
  - '~'
  text_symbols: " %,0123456789=abcdefghijklmnopqrstuvwxyz\xAA\xE0\xE1\xE2\xE3\xE7\xE8\
    \xE9\xEA\xED\xEE\xEF\xF3\xF4\xF5\xF6\xFA\xFB\xFC\xFD"
training:
  batch_size: 8
  batch_size_val: 8
  checkpoint_steps: 2000
  epochs: 10
  generate_steps: 1000
  learning_rate: 0.0001
  n_generate_samples: 10
  scheduler_plateau_factor: 0.5
  scheduler_plateau_patience: 10
  store_phoneme_dict_in_model: true
  validate_steps: 1000
  warmup_steps: 100
